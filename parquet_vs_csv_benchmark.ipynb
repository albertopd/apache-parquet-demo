{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a110242c",
   "metadata": {},
   "source": [
    "# Apache Parquet vs CSV - Benchmarking and Visualization\n",
    "\n",
    "This notebook demonstrates the advantages of using Apache Parquet over traditional CSV files for tabular data storage and analytics. Using a real-world books dataset, we compare file sizes and query performance between CSV and Parquet formats (including compressed and partitioned variants). Visualizations and benchmarks illustrate how Parquet can significantly reduce storage requirements and speed up data processing, especially for columnar queries and filtered reads."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8c7028",
   "metadata": {},
   "source": [
    "### 1. Setup and Data Preparation\n",
    "Load necessary libraries, setup paths and load dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9357f3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Path setup\n",
    "csv_file = \"data/books.csv\"\n",
    "parquet_default = \"data/books_default.parquet\"\n",
    "parquet_compressed = \"data/books_compressed.parquet\"\n",
    "parquet_partitioned = \"data/books_partitioned\"\n",
    "\n",
    "# Load CSV\n",
    "print(\"Loading CSV...\")\n",
    "df = pd.read_csv(csv_file)\n",
    "print(f\"Rows: {len(df)}, Columns: {len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7262a83c",
   "metadata": {},
   "source": [
    "### 2. Export Dataset to Different Parquet Formats\n",
    "- Parquet with default settings.\n",
    "- Parquet with ZStandard compression.\n",
    "- Parquet with ZStandard compression and partitioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5901063",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Exporting books CSV dataset to Parquet files...\")\n",
    "\n",
    "df.to_parquet(parquet_default, engine=\"fastparquet\", index=False)\n",
    "print(f\"Parquet file created with default settings: {parquet_default}\")\n",
    "\n",
    "df.to_parquet(parquet_compressed, engine=\"fastparquet\", compression=\"zstd\", index=False)\n",
    "print(f\"Parquet file created with ZStandard compression: {parquet_compressed}\")\n",
    "\n",
    "df.to_parquet(parquet_partitioned, engine=\"fastparquet\", compression=\"zstd\", partition_cols=[\"language\"], index=False)\n",
    "print(f\"Parquet files created with ZStandard compression and partitioning by language: {parquet_partitioned}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ab876f",
   "metadata": {},
   "source": [
    "### 3. Compare File Sizes\n",
    "Calculate and visualize file sizes for CSV and different Parquet formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8169b7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare File Sizes\n",
    "def file_size(path):\n",
    "    if os.path.isdir(path):\n",
    "        return sum(os.path.getsize(os.path.join(root, f))\n",
    "                   for root, _, files in os.walk(path) for f in files)\n",
    "    else:\n",
    "        return os.path.getsize(path)\n",
    "\n",
    "sizes = {\n",
    "    \"CSV\": file_size(csv_file),\n",
    "    \"Parquet (default)\": file_size(parquet_default),\n",
    "    \"Parquet (compressed)\": file_size(parquet_compressed),\n",
    "    \"Parquet (comp/partioned)\": file_size(parquet_partitioned),\n",
    "}\n",
    "\n",
    "sizes_mb = {k: v/1024/1024 for k, v in sizes.items()}\n",
    "csv_size = sizes_mb[\"CSV\"]\n",
    "\n",
    "print(\" File Size Comparison \".center(51, '='))\n",
    "\n",
    "for name, size in sizes_mb.items():\n",
    "    if name == \"CSV\":\n",
    "        print(f\"{name:<25} {size:.2f} MB\")\n",
    "    else:\n",
    "        reduction = (1 - size / csv_size) * 100\n",
    "        print(f\"{name:<25} {size:.2f} MB  ({reduction:.1f}% smaller)\")\n",
    "\n",
    "# Plot file size comparison\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.bar(sizes_mb.keys(), sizes_mb.values(), color=[\"#f39c12\",\"#2980b9\",\"#8e44ad\",\"#27ae60\"])\n",
    "plt.ylabel(\"File Size (MB)\")\n",
    "plt.title(\"File Size Comparison - CSV vs Parquet\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb96297",
   "metadata": {},
   "source": [
    "### 4. Compare Performance\n",
    "Measure and visualize read times for CSV and different Parquet formats, including filtered queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ddb163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for benchmarking\n",
    "def benchmark(description, func):\n",
    "    \"\"\"Runs a benchmark measuring execution time and memory usage.\"\"\"\n",
    "    start = time.time()\n",
    "    _ = func()\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    print(f\"{description:<45} \"\n",
    "          f\"Time: {elapsed:.3f}s\")\n",
    "    return elapsed\n",
    "\n",
    "def print_percentage_faster(csv_benchmark, parquet_benchmark):\n",
    "    reduction = (1 - parquet_benchmark / csv_benchmark) * 100\n",
    "    print(f\" Parquet is {reduction:.1f}% faster than CSV\".rjust(58, \"=\"))\n",
    "\n",
    "benchmarks = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f4d600",
   "metadata": {},
   "source": [
    "#### 4.1 First Case: Load Full Dataset\n",
    "Benchmark read times when reading the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d4fe29",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" Load Full Dataset \".center(58, \"=\"))\n",
    "\n",
    "benchmarks[\"CSV - Full Load\"] = benchmark(\n",
    "    \"CSV\", \n",
    "    lambda: pd.read_csv(csv_file))\n",
    "\n",
    "benchmarks[\"Parquet - Full Load\"] = benchmark(\n",
    "    \"Parquet\", \n",
    "    lambda: pd.read_parquet(parquet_default, engine=\"fastparquet\"))\n",
    "\n",
    "print_percentage_faster(benchmarks[\"CSV - Full Load\"], benchmarks[\"Parquet - Full Load\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4613a3f5",
   "metadata": {},
   "source": [
    "#### 4.2 Second Case: Column pruning (title, author, rating)\n",
    "Benchmark read times when reading only specific columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1db8669",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \" Load Subset of Columns (title, author, rating) \".center(58, \"=\"))\n",
    "\n",
    "benchmarks[\"CSV - Subset Columns\"] = benchmark(\n",
    "    \"CSV - Column Subset\", \n",
    "    lambda: pd.read_csv(csv_file, usecols=[\"title\", \"author\", \"rating\"]))\n",
    "\n",
    "benchmarks[\"Parquet - Column Pruning\"] = benchmark(\n",
    "    \"Parquet - Column Pruning\", \n",
    "    lambda: pd.read_parquet(parquet_default, columns=[\"title\", \"author\", \"rating\"], engine=\"fastparquet\"))\n",
    "\n",
    "print_percentage_faster(benchmarks[\"CSV - Subset Columns\"], benchmarks[\"Parquet - Column Pruning\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946c9fc7",
   "metadata": {},
   "source": [
    "#### 4.3 Third Case: Predicate pushdown (rating > 4.5)\n",
    "Benchmark read times when applying a filter on the dataset after loading (CSV) vs predicate pushdown (Parquet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e55bda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \" Load Rows Where: rating > 4.5 \".center(58, \"=\"))\n",
    "\n",
    "benchmarks[\"CSV - Filter Rows\"] = benchmark(\n",
    "    \"CSV - Filter Rows After Load\", \n",
    "    lambda: pd.read_csv(csv_file)[lambda d: d[\"rating\"] > 4.5])\n",
    "\n",
    "benchmarks[\"Parquet - Predicate Pushdown\"] = benchmark(\n",
    "    \"Parquet - Predicate Pushdown\", \n",
    "    lambda: pd.read_parquet(parquet_default, filters=[(\"rating\", \">\", 4.5)], engine=\"fastparquet\"))\n",
    "\n",
    "print_percentage_faster(benchmarks[\"CSV - Filter Rows\"], benchmarks[\"Parquet - Predicate Pushdown\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a42488",
   "metadata": {},
   "source": [
    "#### 4.4 Fourth Case: Partition pruning (English books only)\n",
    "Benchmark read times when applying a filter on the dataset after loading (CSV) vs loading a partitioned column (Parquet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a6a297",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \" Load Rows Where: language == 'English' \".center(58, \"=\"))\n",
    "\n",
    "benchmarks[\"CSV - Filter by Language\"] = benchmark(\n",
    "    \"CSV - Filter Rows After Load\", \n",
    "    lambda: pd.read_csv(csv_file)[lambda d: d[\"language\"] == \"English\"])\n",
    "\n",
    "benchmarks[\"Parquet - Read Language Partition\"] = benchmark(\n",
    "    \"Parquet - Partition Pruning\", \n",
    "    lambda: pd.read_parquet(os.path.join(parquet_partitioned, \"language=English\"), engine=\"fastparquet\"))\n",
    "\n",
    "print_percentage_faster(benchmarks[\"CSV - Filter by Language\"], benchmarks[\"Parquet - Read Language Partition\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9719bca",
   "metadata": {},
   "source": [
    "### 5. Benchmark Summary\n",
    "Summarize and visualize all benchmark results for easy comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cda0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group benchmarks by case\n",
    "cases = [\n",
    "    (\"Full Load\", \"CSV - Full Load\", \"Parquet - Full Load\"),\n",
    "    (\"Column Pruning\", \"CSV - Subset Columns\", \"Parquet - Column Pruning\"),\n",
    "    (\"Predicate Pushdown\", \"CSV - Filter Rows\", \"Parquet - Predicate Pushdown\"),\n",
    "    (\"Partition Pruning\", \"CSV - Filter by Language\", \"Parquet - Read Language Partition\"),\n",
    "]\n",
    "\n",
    "csv_times = [benchmarks[case[1]] for case in cases]\n",
    "parquet_times = [benchmarks[case[2]] for case in cases]\n",
    "labels = [case[0] for case in cases]\n",
    "x = np.arange(len(labels))\n",
    "width = 0.35\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.bar(x - (width / 2), csv_times, width, label='CSV', color=\"#f39c12\")\n",
    "plt.bar(x + (width / 2), parquet_times, width, label='Parquet', color=\"#27ae60\")\n",
    "plt.ylabel(\"Time (seconds)\")\n",
    "plt.title(\"Performance Comparison - CSV vs Parquet\")\n",
    "plt.xticks(x, labels)\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1, 1))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
