{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a110242c",
   "metadata": {},
   "source": [
    "# Benchmarking Apache Parquet vs CSV\n",
    "\n",
    "This notebook demonstrates the advantages of using Apache Parquet over traditional CSV files for tabular data storage and analytics. Using a real-world books dataset, we compare file sizes and query performance between CSV and Parquet formats (including compressed and partitioned variants). Visualizations and benchmarks illustrate how Parquet can significantly reduce storage requirements and speed up data processing, especially for columnar queries and filtered reads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b806a528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Path setup\n",
    "csv_file = \"data/books.csv\"\n",
    "parquet_default = \"data/books_default.parquet\"\n",
    "parquet_compressed = \"data/books_compressed.parquet\"\n",
    "parquet_partitioned = \"data/books_partitioned\"\n",
    "\n",
    "# Load CSV\n",
    "print(\"Loading CSV...\")\n",
    "df = pd.read_csv(csv_file)\n",
    "print(f\"Rows: {len(df)}, Columns: {len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5901063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to Parquet formats\n",
    "print(\"Exporting books CSV dataset to Parquet files...\")\n",
    "\n",
    "df.to_parquet(parquet_default, engine=\"fastparquet\", index=False)\n",
    "print(f\"Parquet file created with default settings: {parquet_default}\")\n",
    "\n",
    "df.to_parquet(parquet_compressed, engine=\"fastparquet\", compression=\"gzip\", index=False)\n",
    "print(f\"Parquet file created with GZip compression: {parquet_compressed}\")\n",
    "\n",
    "df.to_parquet(parquet_partitioned, engine=\"fastparquet\", compression=\"gzip\", partition_cols=[\"language\"], index=False)\n",
    "print(f\"Parquet files created with GZip compression and partitioning by language: {parquet_partitioned}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8169b7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare File Sizes\n",
    "def file_size(path):\n",
    "    if os.path.isdir(path):\n",
    "        return sum(os.path.getsize(os.path.join(root, f))\n",
    "                   for root, _, files in os.walk(path) for f in files)\n",
    "    else:\n",
    "        return os.path.getsize(path)\n",
    "\n",
    "sizes = {\n",
    "    \"CSV\": file_size(csv_file),\n",
    "    \"Parquet (default)\": file_size(parquet_default),\n",
    "    \"Parquet (compressed)\": file_size(parquet_compressed),\n",
    "    \"Parquet (comp/partioned)\": file_size(parquet_partitioned),\n",
    "}\n",
    "\n",
    "sizes_mb = {k: v/1024/1024 for k, v in sizes.items()}\n",
    "\n",
    "print(\" File Size Comparison \".center(51, '='))\n",
    "csv_size = sizes_mb[\"CSV\"]\n",
    "\n",
    "for name, size in sizes_mb.items():\n",
    "    if name == \"CSV\":\n",
    "        print(f\"{name:<25} {size:.2f} MB\")\n",
    "    else:\n",
    "        reduction = (1 - size / csv_size) * 100\n",
    "        print(f\"{name:<25} {size:.2f} MB  ({reduction:.1f}% smaller)\")\n",
    "\n",
    "# Plot file size comparison\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(sizes_mb.keys(), sizes_mb.values(), color=[\"#f39c12\",\"#2980b9\",\"#8e44ad\",\"#27ae60\"])\n",
    "plt.ylabel(\"File Size (MB)\")\n",
    "plt.title(\"File Size Comparison - CSV vs Parquet\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a629cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Benchmarks\n",
    "def benchmark(description, func):\n",
    "    \"\"\"Runs a benchmark measuring execution time and memory usage.\"\"\"\n",
    "    start = time.time()\n",
    "    _ = func()\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    print(f\"{description:<45} \"\n",
    "          f\"Time: {elapsed:.3f}s\")\n",
    "    return elapsed\n",
    "\n",
    "benchmarks = {}\n",
    "\n",
    "# Case 1: Load full dataset\n",
    "print(\" Load Full Dataset \".center(58, \"=\"))\n",
    "benchmarks[\"CSV - Full Load\"] = benchmark(\n",
    "    \"CSV\", \n",
    "    lambda: pd.read_csv(csv_file))\n",
    "benchmarks[\"Parquet - Full Load\"] = benchmark(\n",
    "    \"Parquet\", \n",
    "    lambda: pd.read_parquet(parquet_default, engine=\"fastparquet\"))\n",
    "\n",
    "# Case 2: Column pruning (title, author, rating)\n",
    "print(\"\\n\" + \" Load Subset of Columns (title, author, rating) \".center(58, \"=\"))\n",
    "benchmarks[\"CSV - Subset Columns\"] = benchmark(\n",
    "    \"CSV - Column Subset\", \n",
    "    lambda: pd.read_csv(csv_file, usecols=[\"title\", \"author\", \"rating\"]))\n",
    "benchmarks[\"Parquet - Column Pruning\"] = benchmark(\n",
    "    \"Parquet - Column Pruning\", \n",
    "    lambda: pd.read_parquet(parquet_default, columns=[\"title\", \"author\", \"rating\"], engine=\"fastparquet\"))\n",
    "\n",
    "# Case 3: Predicate pushdown (rating > 4.5)\n",
    "print(\"\\n\" + \" Load Rows Where: rating > 4.5 \".center(58, \"=\"))\n",
    "benchmarks[\"CSV - Filter Rows\"] = benchmark(\n",
    "    \"CSV - Filter Rows After Load\", \n",
    "    lambda: pd.read_csv(csv_file)[lambda d: d[\"rating\"] > 4.5])\n",
    "benchmarks[\"Parquet - Predicate Pushdown\"] = benchmark(\n",
    "    \"Parquet - Predicate Pushdown\", \n",
    "    lambda: pd.read_parquet(parquet_default, filters=[(\"rating\", \">\", 4.5)], engine=\"fastparquet\"))\n",
    "\n",
    "# Case 4: Partition pruning (English books only)\n",
    "print(\"\\n\" + \" Load Rows Where: language == 'English' \".center(58, \"=\"))\n",
    "benchmarks[\"CSV - Filter by Language\"] = benchmark(\n",
    "    \"CSV - Filter Rows After Load\", \n",
    "    lambda: pd.read_csv(csv_file)[lambda d: d[\"language\"] == \"English\"])\n",
    "benchmarks[\"Parquet - Read Language Partition\"] = benchmark(\n",
    "    \"Parquet - Partition Pruning\", \n",
    "    lambda: pd.read_parquet(os.path.join(parquet_partitioned, \"language=English\"), engine=\"fastparquet\"))\n",
    "\n",
    "# Plot Benchmark Results\n",
    "names = list(benchmarks.keys())\n",
    "values = list(benchmarks.values())\n",
    "colors = [\"#bc1a1a\", \"#16a085\"]\n",
    "bar_colors = [colors[i % len(colors)] for i in range(len(names))]\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.barh(names, values, color=bar_colors)\n",
    "plt.xlabel(\"Time (seconds)\")\n",
    "plt.title(\"Performance Comparison - CSV vs Parquet\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
